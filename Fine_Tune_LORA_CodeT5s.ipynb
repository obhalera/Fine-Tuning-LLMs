{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ6alEtyVYbG"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets einops wandb\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, LoraModel, PeftModel, get_peft_model\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "GHwOlkLmVhwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.01\n",
        "LORA_R = 16\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q\",\n",
        "    \"v\",\n",
        "]\n",
        "BATCH_SIZES = [8,16,32]\n",
        "LEARNING_RATE_MIN = 1e-6\n",
        "LEARNING_RATE_MAX = 1e-4\n",
        "WEIGHT_DECAY = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
        "NUM_EXPERIMENTS = 15\n",
        "PATIENCE = 3\n",
        "TOP_K = 30\n",
        "TOP_P = 0.95"
      ],
      "metadata": {
        "id": "8dsZEi3YXoGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "swm_F8gbVqsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Dataset...\", split = 'train')"
      ],
      "metadata": {
        "id": "KoocCb-5VpYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed = 42)\n",
        "dataset = dataset.train_test_split(test_size = 0.2)\n",
        "training_set = dataset[\"train\"]\n",
        "auxillary_set = dataset[\"test\"].train_test_split(test_size = 0.5)\n",
        "validation_set = auxillary_set[\"train\"]\n",
        "test_set = auxillary_set[\"test\"]"
      ],
      "metadata": {
        "id": "3Te0FNx1V6Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = training_set.shuffle(seed = 42)\n",
        "validation_set = validation_set.shuffle(seed = 42)\n",
        "test_set = test_set.shuffle(seed = 42)"
      ],
      "metadata": {
        "id": "CHtm6eJOXJgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "UFd9GL06WLXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_examples(examples):\n",
        "  input_sql = [example for example in examples['Instruction']]\n",
        "  output_gql = [example for example in examples['Output']]\n",
        "  model_inputs = tokenizer(input_sql, text_target=output_gql, max_length=128, truncation=True)\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "IQzqDqYTWWFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_training_set = training_set.map(preprocess_examples, batched=True)\n",
        "tokenized_validation_set = validation_set.map(preprocess_examples, batched=True)"
      ],
      "metadata": {
        "id": "pCRanpI-WbE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model='Salesforce/codet5-small')"
      ],
      "metadata": {
        "id": "4JDp4sBDXffn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    peft_type = \"LORA\",\n",
        "    r = LORA_R,\n",
        "    lora_alpha = LORA_ALPHA,\n",
        "    target_modules = LORA_TARGET_MODULES,\n",
        "    lora_dropout = LORA_DROPOUT,\n",
        "    bias = \"none\",\n",
        "    task_type = \"SEQ_2_SEQ_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "nh81GNFSXhnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\").to(device)\n",
        "model = get_peft_model(base_model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "GNYqLRYVX7dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method':'random'\n",
        "}\n",
        "parameters_dict = {\n",
        "    'batch_size':{\n",
        "        'values': BATCH_SIZES\n",
        "    },\n",
        "    'learning_rate':{\n",
        "        'distribution':'log_uniform_values',\n",
        "        'min': MIN_LEARNING_RATE,\n",
        "        'max':MAX_LEARNING_RATE\n",
        "    },\n",
        "    'weight_decay':{\n",
        "        'values':WEIGHT_DECAY\n",
        "    }\n",
        "}\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "metadata": {
        "id": "l8EYCdGAYPEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key = 'key')\n",
        "sweep_id = wandb.sweep(sweep_config, project='project-name')"
      ],
      "metadata": {
        "id": "I5NXQNn4Yxv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStoppingCallback(PATIENCE)"
      ],
      "metadata": {
        "id": "0VkBzqwrZCq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config=None):\n",
        "  with wandb.init(config=config):\n",
        "    config = wandb.config\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "      report_to = 'wandb',\n",
        "      output_dir =\"./results\",\n",
        "      learning_rate = config.learning_rate,\n",
        "      evaluation_strategy = \"epoch\",\n",
        "      per_device_train_batch_size = config.batch_size,\n",
        "      per_device_eval_batch_size = 8,\n",
        "      weight_decay = config.weight_decay,\n",
        "      save_total_limit = 3,\n",
        "      num_train_epochs = 150,\n",
        "      logging_strategy = \"epoch\",\n",
        "      logging_steps = 1,\n",
        "      fp16 = True,\n",
        "      save_strategy = \"epoch\",\n",
        "      save_steps = 1,\n",
        "      metric_for_best_model = \"eval_loss\",\n",
        "      load_best_model_at_end = True\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        train_dataset = tokenized_training_set,\n",
        "        eval_dataset = tokenized_validation_set,\n",
        "        tokenizer = tokenizer,\n",
        "        data_collator = data_collator,\n",
        "        callbacks = [early_stop]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "ge6zbB3DY5vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count = NUM_EXPERIMENTS)"
      ],
      "metadata": {
        "id": "FWSy7Wa7ZdAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = \"Some Random index into the test set\"\n",
        "inputs = tokenizer(test_set[idx]['Instruction'], max_length = 128, truncation = True, return_tensors = \"pt\").input_ids.to(device)\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 40, do_sample = True, top_k = TOP_K, top_p = TOP_P)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "3JJFTcDpZqL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "gDWmRYDEaKK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"Some name...\")\n",
        "tokenizer.push_to_hub(\"Some name...)"
      ],
      "metadata": {
        "id": "iMuNLBlQaMq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9GuPNz5zaZcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}